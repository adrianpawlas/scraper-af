name: Daily Fashion Product Scrape

on:
  # Run daily at midnight UTC
  schedule:
    - cron: '0 0 * * *'

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      max_products:
        description: 'Maximum products to scrape (default: 50)'
        required: false
        default: '50'
        type: string
      dry_run:
        description: 'Run in dry-run mode (no database writes)'
        required: false
        default: true
        type: boolean

  # Run on pushes to main branch (optional, for testing)
  push:
    branches: [ main, master ]
    paths:
      - '.github/workflows/daily-scrape.yml'

env:
  PYTHON_VERSION: '3.11'

jobs:
  scrape:
    runs-on: ubuntu-latest

    # Set timeout to prevent hanging
    timeout-minutes: 30

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Playwright browsers
      run: playwright install --with-deps chromium

    - name: Create cache directories
      run: |
        mkdir -p cache/embeddings
        mkdir -p logs

    - name: Run basic tests
      run: python test_basic.py

    - name: Run scraper
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        MAX_PRODUCTS: ${{ github.event.inputs.max_products || '50' }}
        DRY_RUN: ${{ github.event.inputs.dry_run == true && '--dry-run' || '' }}
      run: |
        echo "Starting daily scrape..."
        echo "Max products: $MAX_PRODUCTS"
        echo "Dry run: $DRY_RUN"

        # Run scraper with limits suitable for CI
        python main.py \
          --max-products "$MAX_PRODUCTS" \
          $DRY_RUN \
          2>&1 | tee scrape_log.txt

        echo "Scrape completed"

    - name: Generate summary
      if: always()
      run: |
        echo "## ðŸ¤– Daily Scrape Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Timestamp:** $(date)" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY

        if [ -f scrape_log.txt ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Last 20 lines of log:**" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          tail -20 scrape_log.txt >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Configuration:**" >> $GITHUB_STEP_SUMMARY
        echo "- Max products: ${{ github.event.inputs.max_products || '50' }}" >> $GITHUB_STEP_SUMMARY
        echo "- Dry run: ${{ github.event.inputs.dry_run || 'true' }}" >> $GITHUB_STEP_SUMMARY

    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: scrape-logs-${{ github.run_number }}
        path: |
          scrape_log.txt
          logs/
        retention-days: 7

    - name: Notify on failure
      if: failure()
      run: |
        echo "## âŒ Daily Scrape Failed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "The automated daily scrape failed. Check the logs for details." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
